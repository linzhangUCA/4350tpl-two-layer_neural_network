{"cells":[{"cell_type":"markdown","metadata":{"id":"R3uAK0924pwe"},"source":["# Two-Layer Neural Network\n","**Due: Mondy, 09/26/2022, 2:15 PM**\n","\n","Welcome to your third assignment. You will build your first neural network in this assignment. \n","\n","Contents:\n","1. (5%) Exercise 1:  Extract key properties\n","2. (10%) Exercise 2.1: Initialize neural network parameters\n","3. (20%) Exercise 2.2: Forward propagation\n","4. (5%) Exercise 2.3: Compute Cost\n","5. (30%) Exercise 2.4: Compute gradients\n","6. (10%) Exerise 2.5: Update parameters\n","7. (10%) Exercise 2.6: Train neural network\n","8. (5%) Exercise 2.7: Classification\n","9. (5%) Exercise 4: Train neural networks on new datasets\n","\n","**Instructions:**\n","- The code between the ### START CODE HERE ### and ### END CODE HERE ### comments will be graded.\n","- Change variable names at your own risk. Make sure you understand what you are doing.\n","- Avoid using for-loops and while-loops, unless you are explicitly asked to do so.\n","\n","**You will learn how to:**\n","- Implement a binary classification neural network with a single hidden layer.\n","- Parameterize neural network with numpy arrays.\n","- Use units with a non-linear activation function, such as tanh "]},{"cell_type":"markdown","metadata":{"id":"pOYO1Lov4pwj"},"source":["## 1 - Dataset ##\n","\n","The following code will load a imaginary dataset. Features: `X` is a matrix made with 2-dimensional coordinates of datapoints on a plane. Labels: `y` is a binary-class vector.  \n","You can visualize the dataset by running following code block. The datapoints make a \"flower\" pattern with red dots (label **y=0**) and some blue (**y=1**) points. \n","\n","**Your goal is to build a model to distinguish these datapoints.** In other words, we want to build a classifier to define red regions and blue regions in this plane."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X1YxPV3P4pwk"},"outputs":[],"source":["# Package imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegressionCV\n","from utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n","\n","%matplotlib inline\n","np.random.seed(1) # set a seed so that the results are consistent\n","\n","# Load dataset\n","X, y = load_planar_dataset()\n","# Visualize the data:\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral);\n"]},{"cell_type":"markdown","metadata":{"id":"o7crQbiP4pwl"},"source":["The dataset contains:\n","- a matrix `X` that contains your features (x1, x2)\n","- a vector `y` that contains your labels (red:0 / blue:1).\n","\n","As always, lets get a better sense of what our data looks like. \n","\n","**(5%) Exercise 1:  Extract key properties**\n","\n","Determine data and label dimensions. Also find out number of examples and number of features."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VXc1ThOj4pwm"},"outputs":[],"source":["### START CODE HERE ### (≈ 4 lines of code)\n","assert X.shape[0] == y.shape[0]\n","dim_X = None\n","dim_y = None\n","M = None  # number of training examples\n","N = None  # number of features/variables for each example\n","### END CODE HERE ###\n","\n","print (f\"The dimension of data is: {dim_X}\")\n","print (f\"The dimension of label is: {dim_y}\")\n","print (f\"This dataset has {M} examples\")\n","print (f\"Each example has {N} features\")"]},{"cell_type":"markdown","metadata":{"id":"CSOsHsF84pwn"},"source":["**Expected Output**:\n",">\n","```console\n","The dimension of data is: (400, 2)\n","The dimension of label is: (400, 1)\n","This dataset has 400 examples\n","Each example has 2 features\n","```"]},{"cell_type":"markdown","metadata":{"id":"vMxp9i3o4pwn"},"source":["Before building a neural network, lets first see how logistic regression performs on this problem. You can use sklearn's built-in functions to do thit. Run the code block below to train a logistic regression classifier on the dataset.\n","> You can use this function for the task in the previous assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xhSdXr9n4pwo"},"outputs":[],"source":["# Train the logistic regression classifier\n","clf = LogisticRegressionCV()\n","clf.fit(X, y.reshape(-1))\n","\n","# Plot the decision boundary for logistic regression\n","plot_decision_boundary(lambda x: clf.predict(x), X, y)\n","plt.title(\"Logistic Regression\")\n","\n","# Print accuracy\n","LR_predictions = clf.predict(X)\n","accuracy = (np.dot(y.reshape(-1), LR_predictions) + np.dot((1-y.reshape(-1)), (1-LR_predictions))) / float(y.shape[0])\n","print (f\"Logistic Regression Accuracy: {accuracy*100}%\")"]},{"cell_type":"markdown","metadata":{"id":"NvTegOB64pwp"},"source":["> The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better. Let's try this now! "]},{"cell_type":"markdown","metadata":{"id":"hzIJe0H84pwp"},"source":["## 2 - Neural Network model\n","\n","Logistic regression did not work well on the \"flower dataset\". Let's build a two-layer neural network model to tackle this problem.\n","\n","Please refer to following general workflow to build such a neural network:\n","1. Determine number of neurons in the hidden layer. \n","2. Initialize the neural networks parameters, include:\n","    - weights and biases connect inputs to hidden layer\n","    - weights and bias connect hiddent layer to output layer.\n","3. Implement forward propagation to make predictions\n","4. Compute loss between predictions and groundtruths\n","5. Implement backward propagation to calculate the gradients\n","6. Update parameters (weights and biases) using gradient descent algorithm"]},{"cell_type":"markdown","metadata":{"id":"VeWEY68S4pwq"},"source":["### 2.1 - Initialization\n","Parameters determine the accurcy of the classifier. Before we can use gradient descent to approach the optimal values, let's first initialize them to values close to 0. \n","\n","**(10%) Exercise 2.1: Initialize neural network parameters**\n","\n","Complete function `init_params()` to initialize parameters based on the dimension of input data and the hidden layer size. The output of this function will be a dictionary storing all these parameters. \n","\n","- You can initialize all the bias terms with 0.\n","- Hint: to create an array with small values, consider use something like `np.random.randn(a,b) * 0.01` \n","- Please refer to the 6th [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the dimensions of the parameters. "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4LzKZa6d4pwr"},"outputs":[],"source":["def init_params(dim_input, dim_layer):\n","    \"\"\"\n","    Argument:\n","        dim_input -- input feature dimension, N\n","        dim_layer -- hidden layer dimension, L\n","    \n","    Returns:\n","        params -- python dictionary containing your parameters:\n","            W1 -- 1st layer weight matrix of shape (L, N)\n","            b1 -- 1st layer bias vector of shape (1, L)\n","            w2 -- 2nd layer weight vector of shape (1, L)\n","            b2 -- 2nd layer bias vector of shape (1, 1)\n","    \"\"\"\n","        \n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = None\n","    b1 = None\n","    w2 = None\n","    b2 = None\n","    ### END CODE HERE ###\n","    \n","    assert (W1.shape == (dim_layer, dim_input))\n","    assert (b1.shape == (1, dim_layer))\n","    assert (w2.shape == (1, dim_layer))\n","    assert (b2.shape == (1, 1))\n","    \n","    params = {\n","        'W1': W1,\n","        'b1': b1,\n","        'w2': w2,\n","        'b2': b2\n","    }\n","    \n","    return params\n","\n","\n","# test\n","np.random.seed(1) # set a seed so that the results are consistent\n","N, L = 2, 4\n","\n","parameters = init_params(N, L)\n","print(f\"W1 = {parameters['W1']}\")\n","print(f\"b1 = {parameters['b1']}\")\n","print(f\"w2 = {parameters['w2']}\")\n","print(f\"b2 = {parameters['b2']}\")"]},{"cell_type":"markdown","metadata":{"id":"rrj6AHs74pwr"},"source":["**Expected Output**:\n",">\n","```console\n","W1 = [[ 0.01624345 -0.00611756]\n"," [-0.00528172 -0.01072969]\n"," [ 0.00865408 -0.02301539]\n"," [ 0.01744812 -0.00761207]]\n","b1 = [[ 0. -0.  0. -0.]]\n","w2 = [[-0.00322417 -0.00384054  0.01133769 -0.01099891]]\n","b2 = [[-0.]]\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"a48zHxWO4pws"},"source":["### 2.2 - Forward Propagation\n","With parameters, we are able to make predictions. We'll use the predictions (random guesses at this moment) to compute losses and cost later.\n","\n","**(20%) Exercise 2.2: Forward propagation**\n","\n","Complete function `forward()` to compute predictions and intermediate results in the forward propagation. Feed data matrix, `X` and parameter dictionary, `params` to predict probabilities of the classes, `yhat`.\n","- Use hyperbolic tangent function to compute values of neurons in the hidden layer, `X1` . This function comes with NumPy library.\n","- Use sigmoid function to compute the predictions. You are welcome to use the imported `sigmoid()` function.\n","- Store intermediate results in \"`cache` dictionary. `cache` will be given as an input to the backpropagation function.\n","- Refer to the 7th [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the mathematical representation of forward propagation.\n","- Refer to the 6th [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the shapes of the arrays.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8NAi0UKh4pws"},"outputs":[],"source":["def forward(X, params):\n","    \"\"\"\n","    Argument:\n","        X -- data matrix of size (M, N)\n","        params -- dictionary containing the parameters\n","    \n","    Returns:\n","        yhat -- The sigmoid output of the second activation\n","        cache -- a dictionary containing intermediate results:\n","            Z1 -- hidden layer, a matrix before activation, (M, L)\n","            X1 -- hidden layer, a matrix after activation, (M, L)\n","            z2 -- output layer, a vector before activation (M, 1)\n","            yhat -- output layer, a vector after activation, (M, 1)\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 8 lines of code)\n","    # Retrieve each parameter from the dictionary \"params\"\n","    W1 = None\n","    b1 = None\n","    w2 = None\n","    b2 = None\n","    # Implement Forward Propagation to calculate yhat (probabilities)\n","    Z1 = None\n","    X1 = None\n","    z2 = None\n","    yhat = None\n","    ### END CODE HERE ###\n","        \n","    assert(yhat.shape == (X.shape[0], 1))\n","    \n","    cache = {\n","        'Z1': Z1,\n","        'X1': X1,\n","        'z2': z2,\n","        'yhat': yhat,\n","    }\n","    \n","    return yhat, cache\n","\n","\n","# test\n","np.random.seed(1) # set a seed so that the results are consistent\n","X_assess = np.random.randn(3, 2)\n","parameters = {\n","    'W1': np.array([[-0.00416758, -0.00056267],\n","        [-0.02136196,  0.01640271],\n","        [-0.01793436, -0.00841747],\n","        [ 0.00502881, -0.01245288]]),\n","    'w2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n","    'b1': np.random.randn(1, 4),\n","    'b2': np.array([[-1.3]])\n","}\n","\n","yhat, cache = forward(X_assess, parameters)\n","print(f\"Z1 = {cache['Z1']}\")\n","print(f\"X1 = {cache['X1']}\")\n","print(f\"z2 = {cache['z2']}\")\n","print(f\"yhat = {cache['yhat']}\")"]},{"cell_type":"markdown","metadata":{"id":"bhb4hctV4pwt"},"source":["**Expected Output**:\n",">\n","```console\n","Z1 = [[ 1.73838639 -0.80594056  0.29505694 -0.23358372]\n"," [ 1.74761669 -0.76752371  0.3375432  -0.2386649 ]\n"," [ 1.74250012 -0.81744518  0.3228917  -0.21635762]]\n","X1 = [[ 0.94003922 -0.66734479  0.28678255 -0.22942622]\n"," [ 0.94110368 -0.64548722  0.3252823  -0.2342343 ]\n"," [ 0.94051591 -0.67367693  0.31211926 -0.21304371]]\n","z2 = [[-1.3075564 ]\n"," [-1.30766425]\n"," [-1.30698864]]\n","yhat = [[0.21289603]\n"," [0.21287796]\n"," [0.21299119]]\n"," ```"]},{"cell_type":"markdown","metadata":{"id":"WMkknir14pwt"},"source":["### 2.3 - Compute Cost\n","With parameters, we are able to make predictions. We'll use the predictions (random guesses at this moment) to compute losses and cost later.\n","\n","**(5%) Exercise 2.3: Compute Cost**\n","\n","Complete function `compute_cost()` to compute the costs caused by wrong prediction. Feed prediction, `yhat` and groundtruth, `y` to obtain `cost`.\n","- Refer to the 7th [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the mathematical representation of forward propagation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"flES0Uj_4pwt"},"outputs":[],"source":["def compute_cost(yhat, y):\n","    \"\"\"    \n","    Arguments:\n","        yhat -- predict probabilies, vector of shape (M, 1)\n","        y -- \"true\" labels, vector of shape (M, 1)\n","    \n","    Returns:\n","        cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    losses = None\n","    cost = None\n","    ### END CODE HERE ###\n","    \n","    cost = np.squeeze(cost)  # get rid of unwanted dimensions\n","    assert(isinstance(cost, float))\n","    \n","    return cost\n","\n","\n","# test\n","np.random.seed(1)\n","y_assess = (np.random.randn(3, 1) > 0)\n","yhat_assess = np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]).T\n","print(f\"cost = {compute_cost(yhat_assess, y_assess)}\")"]},{"cell_type":"markdown","metadata":{"id":"ZLYVpR-i4pwu"},"source":["**Expected Output**:\n",">\n","```console\n","cost = 0.6930587610394646\n","```"]},{"cell_type":"markdown","metadata":{"id":"JLHpgTP94pwu"},"source":["### 2.4 - Backward Propagation\n","Compute gradients of the neural network's parameters in order to update them based on the directions that could decrease the cost later.\n","\n","**(30%) Exercise 2.4: Compute gradients**\n","\n","Complete function `grad()` to compute the gradients of the cost w.r.t. parameters. Feed parameters, `params`, intermediate results computed in forward propagation, `cache`, data, `X`, true labels, `y` to compute parameters' gradients, `grads`.\n","- Refer to the last [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the mathematical representation of backward propagation.\n","- Refer to the 9th [slide](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0914/nn_p1.pdf) for the mathematical representation of derivative of `tanh` function."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xkSOzq8e4pwv"},"outputs":[],"source":["def grad(params, cache, X, y):\n","    \"\"\"\n","    Implement the backward propagation using the instructions above.\n","    \n","    Arguments:\n","        params -- python dictionary containing nn parameters \n","        cache -- a dictionary containing 'Z1', 'X1', 'z2' and 'yhat'\n","        X -- input data of shape (M, 2)\n","        y -- \"true\" labels vector of shape (M, 1)\n","    \n","    Returns:\n","        grads -- python dictionary containing your gradients with respect to different parameters\n","    \"\"\"\n","    m = X.shape[0]\n","    \n","    ### START CODE HERE ### (≈ 10 lines of code)\n","    # Retrieve W1 and w2 from the dictionary \"params\".\n","    W1 = None\n","    w2 = None        \n","    # Retrieve also X1 and yaht from dictionary \"cache\".\n","    X1 = None\n","    yhat = None    \n","    # Backward propagation: calculate dW1, db1, dw2, db2. \n","    dz2 = None\n","    dw2 = None\n","    db2 = None\n","    dX1 = None\n","    dZ1 = None\n","    dW1 = None\n","    db1 = None\n","    ### END CODE HERE ###\n","    \n","    grads = {'dW1': dW1,\n","             'db1': db1,\n","             'dw2': dw2,\n","             'db2': db2}\n","    \n","    return grads\n","\n","\n","# test\n","np.random.seed(1)\n","X_assess = np.random.randn(3, 2)\n","y_assess = (np.random.randn(3, 1) > 0)\n","parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n","    [-0.02136196,  0.01640271],\n","    [-0.01793436, -0.00841747],\n","    [ 0.00502881, -0.01245288]]),\n","    'w2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n","    'b1': np.zeros((1, 4)),\n","    'b2': np.array([[ 0.]])}\n","\n","cache = {'X1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n","        [-0.05225116,  0.02725659, -0.02646251],\n","        [-0.02009721,  0.0036869 ,  0.02883756],\n","        [ 0.02152675, -0.01385234,  0.02599885]]).T,\n","'yhat': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]).T,\n","'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n","        [-0.05229879,  0.02726335, -0.02646869],\n","        [-0.02009991,  0.00368692,  0.02884556],\n","        [ 0.02153007, -0.01385322,  0.02600471]]).T,\n","'z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]]).T}\n","\n","grads = grad(parameters, cache, X_assess, y_assess)\n","print (f\"dW1 = {grads['dW1']}\")\n","print (f\"db1 = {grads['db1']}\")\n","print (f\"dw2 = {grads['dw2']}\")\n","print (f\"db2 = {grads['db2']}\")"]},{"cell_type":"markdown","metadata":{"id":"Bbs5ZCb34pwv"},"source":["**Expected output**:\n",">\n","```console\n","dW1 = [[ 0.00531892 -0.00324297]\n"," [ 0.00456198 -0.00278272]\n"," [-0.00277127  0.00168846]\n"," [-0.011519    0.00702032]]\n","db1 = [[ 0.00176201  0.00150995 -0.00091736 -0.00381422]]\n","dw2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]\n","db2 = [[-0.16655712]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"f-3dJSV94pww"},"source":["### 2.5 - Gradient Descent\n","Now, we can perform gradient descent algorithm to reduce cost iteratively.\n","\n","**(10%) Exerise 2.5: Update parameters**\n","\n","Complete `update_params()` function to perform gradient descent update on the parameters. Feed in `(dW1, db1, dw2, db2)` in `grads` in order to update `(W1, b1, w2, b2)` in `params`.\n","- General gradient descent rule: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"V9C2cDSD4pww"},"outputs":[],"source":["def update_params(params, grads, learning_rate=1.2):\n","    \"\"\"\n","    Updates parameters using the gradient descent\n","    \n","    Arguments:\n","        params -- python dictionary containing trainable parameters \n","        grads -- python dictionary containing gradients \n","    \n","    Returns:\n","        params -- python dictionary containing updated parameters \n","    \"\"\"\n","    ### START CODE HERE ### (≈ 12 lines of code)\n","    # Retrieve each parameter from the dictionary \"params\"\n","    W1 = None\n","    b1 = None\n","    w2 = None\n","    b2 = None    \n","    # Retrieve each gradient from the dictionary \"grads\"\n","    dW1 = None\n","    db1 = None\n","    dw2 = None\n","    db2 = None\n","    # Update rule for each parameter\n","    W1 = None\n","    b1 = None\n","    w2 = None\n","    b2 = None\n","    ### END CODE HERE ###\n","    \n","    params = {\n","        'W1': W1,\n","        'b1': b1,\n","        'w2': w2,\n","        'b2': b2\n","    }\n","    \n","    return params\n","\n","\n","# test\n","parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n","    [-0.02311792,  0.03137121],\n","    [-0.0169217 , -0.01752545],\n","    [ 0.00935436, -0.05018221]]),\n","'w2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n","'b1': np.array([[ -8.97523455e-07],\n","    [  8.15562092e-06],\n","    [  6.04810633e-07],\n","    [ -2.54560700e-06]]).T,\n","'b2': np.array([[  9.14954378e-05]])}\n","\n","grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n","    [ 0.00082222, -0.00700776],\n","    [-0.00031831,  0.0028636 ],\n","    [-0.00092857,  0.00809933]]),\n","'dw2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n","        -2.55715317e-03]]),\n","'db1': np.array([[  1.05570087e-07],\n","    [ -3.81814487e-06],\n","    [ -1.90155145e-07],\n","    [  5.46467802e-07]]).T,\n","'db2': np.array([[ -1.08923140e-05]])}\n","\n","parameters = update_params(parameters, grads)\n","print(f\"W1 = {parameters['W1']}\")\n","print(f\"b1 = {parameters['b1']}\")\n","print(f\"w2 = {parameters['w2']}\")\n","print(f\"b2 = {parameters['b2']}\")"]},{"cell_type":"markdown","metadata":{"id":"ohaJmwmy4pww"},"source":["**Expected Output**:\n",">\n","```console\n","W1 = [[-0.00643025  0.01936718]\n"," [-0.02410458  0.03978052]\n"," [-0.01653973 -0.02096177]\n"," [ 0.01046864 -0.05990141]]\n","b1 = [[-1.02420756e-06  1.27373948e-05  8.32996807e-07 -3.20136836e-06]]\n","w2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\n","b2 = [[0.00010457]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"tR1Vr0vf4pwx"},"source":["### 2.6 - Integration\n","\n","We have prepared all the building blocks for a neural network. Let's put them together now.\n","\n","**(10%) Exercise 2.6: Train neural network** \n","\n","Train a two-layer (single hidden layer) neural network by put previously built functions in order. Complete `nn_model()` function. Feed Data (`X`), true labels (`y`) and hidden layer size (`dim_layer`) to the function. Output updated parameters (`params`) after `num_iterations` of updates."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"XLsuIdFn4pwx"},"outputs":[],"source":["def nn_model(X, y, dim_layer, num_iterations=10000, print_cost=False):\n","    \"\"\"\n","    Arguments:\n","        X -- data matrix\n","        y -- ground truth labels\n","        dim_layer -- hidden layer size\n","        num_iterations -- Number of iterations in gradient descent loop\n","        print_cost -- if True, print the cost every 1000 iterations\n","    \n","    Returns:\n","        params -- updated parameters\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    \n","    # Initialize parameters\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    params = None\n","    ### END CODE HERE ###\n","    \n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","         \n","        ### START CODE HERE ### (≈ 4 lines of code)\n","        # Forward propagation.\n","        yhat, cache = None \n","        # Compute cost.\n","        cost = None\n","        # Backpropagation (Compute gradients)\n","        grads = None\n","        # Gradient descent parameter update\n","        params = None\n","        ### END CODE HERE ###\n","        \n","        # Print the cost every 1000 iterations\n","        if print_cost and not i % 1000:\n","            print (f\"Cost after iteration {i}: {cost}\")\n","\n","    return params\n","\n","\n","# test \n","np.random.seed(1)\n","X_assess = np.random.randn(3, 2)\n","Y_assess = (np.random.randn(3, 1) > 0)\n","\n","parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n","print(f\"W1 = {parameters['W1']}\")\n","print(f\"b1 = {parameters['b1']}\")\n","print(f\"W2 = {parameters['w2']}\")\n","print(f\"b2 = {parameters['b2']}\")"]},{"cell_type":"markdown","metadata":{"id":"HKNr4MzD4pwx"},"source":["**Expected Output**:\n",">\n","```console\n","Cost after iteration 0: 0.692980\n","Cost after iteration 1000: 0.000329\n","Cost after iteration 2000: 0.000155\n","Cost after iteration 3000: 0.000101\n","Cost after iteration 4000: 0.000074\n","Cost after iteration 5000: 0.000058\n","Cost after iteration 6000: 0.000048\n","Cost after iteration 7000: 0.000041\n","Cost after iteration 8000: 0.000035\n","Cost after iteration 9000: 0.000031\n","W1 = [[ 2.39484555 -0.25752355]\n"," [ 2.42773549 -0.26651015]\n"," [-0.536067    0.10188337]\n"," [-1.35935834  0.14489249]]\n","b1 = [[-0.8024035  -0.82340705  0.09423512  0.40945937]]\n","W2 = [[ 4.48282085  4.61820705 -0.58866038 -1.78642279]]\n","b2 = [[0.08248502]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"4XK3y4ML4pwx"},"source":["### 2.7 Predictions\n","\n","Now, you can use the updated parameters to classify any point in the plane. \n","\n","**(5%) Exercise 2.7: Classification**\n","\n","Complete `predict()` function to enable classification. A prediction needs to be an integer either 0 or 1 to represent the class. \n","- Use 0.5 as the threshold."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"dySQoOz_4pwx"},"outputs":[],"source":["def predict(params, X):\n","    \"\"\"\n","    Given parameters, classify any datapoints\n","    \n","    Arguments:\n","        params -- python dictionary containing nn parameters \n","        X -- datapoints need to be classified\n","    \n","    Returns\n","        pred -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    # Computes class probabilities using forward propagation\n","    yhat, cache = None\n","    # Classifies to 0/1 using 0.5 as the threshold.\n","    pred = None\n","    ### END CODE HERE ###\n","    \n","    return pred\n","\n","\n","# test\n","np.random.seed(0)\n","X_assess = np.random.randn(3, 2)\n","parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n","    [-0.02311792,  0.03137121],\n","    [-0.0169217 , -0.01752545],\n","    [ 0.00935436, -0.05018221]]),\n","    'w2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n","    'b1': np.array([[ -8.97523455e-07],\n","    [  8.15562092e-06],\n","    [  6.04810633e-07],\n","    [ -2.54560700e-06]]).T,\n","    'b2': np.array([[  9.14954378e-05]])}\n","\n","predictions = predict(parameters, X_assess)\n","print(f\"predictions mean = {np.mean(predictions)}\")"]},{"cell_type":"markdown","metadata":{"id":"Rivhx2f34pwy"},"source":["**Expected Output**: \n",">\n","```console\n","predictions mean = 0.6666666666666666\n","```"]},{"cell_type":"markdown","metadata":{"id":"uWpdKD5j4pwy"},"source":["## 3 - Model Deploy\n","\n","It is time to run the neural network model and see how it performs on this planar dataset. Run the following code to test your model with a single hidden layer of 4 neurons."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"id":"SO_CNCUt4pwy"},"outputs":[],"source":["# Build an nn model\n","params = nn_model(X, y, dim_layer=4, num_iterations=10000, print_cost=True)\n","\n","# Print accuracy\n","preds = predict(params, X)\n","print(f\"Accuracy: {np.squeeze((np.dot(y.T, preds) + np.dot((1-y.T), (1-preds))) / y.size * 100)}%\")\n","\n","# Plot the decision boundary\n","plot_decision_boundary(lambda x: predict(params, x), X, y)\n","plt.title(\"Neural Network\")"]},{"cell_type":"markdown","metadata":{"id":"PMcvuxrB4pwy"},"source":["**Expected Output**:\n",">\n","```console\n","Cost after iteration 0: 0.6930928851053969\n","Cost after iteration 1000: 0.2844852924337878\n","Cost after iteration 2000: 0.26911214083616064\n","Cost after iteration 3000: 0.26204655333252974\n","Cost after iteration 4000: 0.2574590704841595\n","Cost after iteration 5000: 0.2542980359370941\n","Cost after iteration 6000: 0.25201305747470276\n","Cost after iteration 7000: 0.25029610576983485\n","Cost after iteration 8000: 0.2489649619459887\n","Cost after iteration 9000: 0.24790590441692756\n","Accuracy: 91.25%\n","```"]},{"cell_type":"markdown","metadata":{"id":"uieWcoMa4pwy"},"source":["Accuracy is largely improved compared to Logistic Regression. This result suggests that neural network model is more expressive than logistic regression model. \n","Run the following code to find out how hidden layer size may affect the decision making. It may take several min minutes depending on hidden layer size. You will observe different behaviors of the model for various hidden layer sizes."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"id":"ihkTURhh4pwz"},"outputs":[],"source":["### FEEL FREE TO MODIFY LINE BELOW ###\n","hidden_layer_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n","### FEEL FREE TO MODIFY LINE ABOVE ###\n","\n","plt.figure(figsize=(16, 32))\n","for i, L in enumerate(hidden_layer_sizes):\n","    plt.subplot(5, 2, i+1)\n","    plt.title('Hidden Layer of size %d' % L)\n","    params = nn_model(X, y, L, num_iterations = 5000)\n","    plot_decision_boundary(lambda x: predict(params, x), X, y)\n","    preds = predict(params, X)\n","    accuracy = np.squeeze((np.dot(y.T, preds) + np.dot((1-y.T), (1-preds))) / y.size * 100)\n","    print (f\"Accuracy for {L} hidden units: {accuracy}%\")"]},{"cell_type":"markdown","metadata":{"id":"fnBlKWmX4pwz"},"source":["## 4 - Train More Neural Networks\n","\n","**(5%) Exercise 4: Train neural networks on new datasets**\n","\n","You can train more neural network models using different datasets. Complete the required coding section below and observe the new classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"id":"V-BdvEeg4pwz"},"outputs":[],"source":["# Load new dataset\n","noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n","datasets = {\"noisy_circles\": noisy_circles,\n","            \"noisy_moons\": noisy_moons,\n","            \"blobs\": blobs,\n","            \"gaussian_quantiles\": gaussian_quantiles}\n","### CHANGE `dname` BELOW TO CHOOSE YOUR DATASET ###\n","dname = \"noisy_moons\"\n","### CHANGE `dname` ABOVE TO CHOOSE YOUR DATASET ###\n","X, y = datasets[dname]\n","y = y%2\n","y = y.reshape((-1, 1))  # preprocess\n","\n","### START CODE HERE ### (~ 1 lines of code)\n","params = None\n","### END CODE HERE ###\n","\n","preds = predict(params, X)\n","accuracy = np.squeeze((np.dot(y.T, preds) + np.dot((1-y.T), (1-preds))) / y.size * 100)\n","print (f\"Accuracy for {hidden_layer_size} hidden units: {accuracy}%\")\n","plot_decision_boundary(lambda x: predict(params, x), X, y)\n"]},{"cell_type":"markdown","metadata":{"id":"XpgDVef44pwz"},"source":["# Congrats on finishing this Assignment!"]}],"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"wRuwL","launcher_item_id":"NI888"},"kernelspec":{"display_name":"Python 3.10.6 64-bit ('3.10.6')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"d270a79710bdef277cbe1980c659a200a6519333aa892b0e4ae637e197c06188"}},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}